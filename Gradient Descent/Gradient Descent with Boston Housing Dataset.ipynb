{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \\[21\\]:\n",
    "\n",
    "    #Import the required libraries.\n",
    "    import matplotlib\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "In \\[22\\]:\n",
    "\n",
    "    # Importing the Boston Housing dataset\n",
    "    from sklearn.datasets import load_boston\n",
    "    boston_data = load_boston()\n",
    "    print(boston_data['DESCR'])\n",
    "\n",
    "    .. _boston_dataset:\n",
    "\n",
    "    Boston house prices dataset\n",
    "    ---------------------------\n",
    "\n",
    "    **Data Set Characteristics:**  \n",
    "\n",
    "        :Number of Instances: 506 \n",
    "\n",
    "        :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
    "\n",
    "        :Attribute Information (in order):\n",
    "            - CRIM     per capita crime rate by town\n",
    "            - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "            - INDUS    proportion of non-retail business acres per town\n",
    "            - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "            - NOX      nitric oxides concentration (parts per 10 million)\n",
    "            - RM       average number of rooms per dwelling\n",
    "            - AGE      proportion of owner-occupied units built prior to 1940\n",
    "            - DIS      weighted distances to five Boston employment centres\n",
    "            - RAD      index of accessibility to radial highways\n",
    "            - TAX      full-value property-tax rate per $10,000\n",
    "            - PTRATIO  pupil-teacher ratio by town\n",
    "            - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
    "            - LSTAT    % lower status of the population\n",
    "            - MEDV     Median value of owner-occupied homes in $1000's\n",
    "\n",
    "        :Missing Attribute Values: None\n",
    "\n",
    "        :Creator: Harrison, D. and Rubinfeld, D.L.\n",
    "\n",
    "    This is a copy of UCI ML housing dataset.\n",
    "    https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
    "\n",
    "\n",
    "    This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
    "\n",
    "    The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
    "    prices and the demand for clean air', J. Environ. Economics & Management,\n",
    "    vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
    "    ...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
    "    pages 244-261 of the latter.\n",
    "\n",
    "    The Boston house-price data has been used in many machine learning papers that address regression\n",
    "    problems.   \n",
    "         \n",
    "    .. topic:: References\n",
    "\n",
    "       - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
    "       - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
    "\n",
    "    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
    "\n",
    "        The Boston housing prices dataset has an ethical problem. You can refer to\n",
    "        the documentation of this function for further details.\n",
    "\n",
    "        The scikit-learn maintainers therefore strongly discourage the use of this\n",
    "        dataset unless the purpose of the code is to study and educate about\n",
    "        ethical issues in data science and machine learning.\n",
    "\n",
    "        In this special case, you can fetch the dataset from the original\n",
    "        source::\n",
    "\n",
    "            import pandas as pd\n",
    "            import numpy as np\n",
    "\n",
    "\n",
    "            data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "            raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "            data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "            target = raw_df.values[1::2, 2]\n",
    "\n",
    "        Alternative datasets include the California housing dataset (i.e.\n",
    "        :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
    "        dataset. You can load the datasets as follows::\n",
    "\n",
    "            from sklearn.datasets import fetch_california_housing\n",
    "            housing = fetch_california_housing()\n",
    "\n",
    "        for the California housing dataset and::\n",
    "\n",
    "            from sklearn.datasets import fetch_openml\n",
    "            housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
    "\n",
    "        for the Ames housing dataset.\n",
    "        \n",
    "      warnings.warn(msg, category=FutureWarning)\n",
    "\n",
    "In \\[23\\]:\n",
    "\n",
    "    # take the boston data\n",
    "    data = boston_data['data']\n",
    "    # we will only work with two of the features: INDUS and RM\n",
    "    x_input = data[:, [2,5]]\n",
    "    y_target = boston_data['target']\n",
    "\n",
    "In \\[24\\]:\n",
    "\n",
    "    #cost function across an entire dataset (X, t):\n",
    "    def cost(w1, w2, b, X, t):\n",
    "        '''\n",
    "        Evaluate the cost function in a non-vectorized manner for \n",
    "        inputs `X` and targets `t`, at weights `w1`, `w2` and `b`.\n",
    "        '''\n",
    "        # TODO: write this!\n",
    "        costs = 0\n",
    "        for i in range(len(t)):\n",
    "            y_i = w1 * X[i, 0] + w2 * X[i, 1] + b\n",
    "            t_i = t[i]\n",
    "            costs += 0.5 * (y_i - t_i) ** 2\n",
    "        return costs / len(t)\n",
    "\n",
    "In \\[25\\]:\n",
    "\n",
    "     #vectorized code looks like this:\n",
    "\n",
    "    def cost_vectorized(w1, w2, b, X, t):\n",
    "        '''\n",
    "        Evaluate the cost function in a vectorized manner for \n",
    "        inputs `X` and targets `t`, at weights `w1`, `w2` and `b`.\n",
    "        '''\n",
    "        # TODO: write this!\n",
    "        N = len(y_target)\n",
    "        w = np.array([w1, w2])\n",
    "        y = np.dot(X, w) + b * np.ones(N)\n",
    "        return np.sum((y - t)**2) / (2.0 * N)\n",
    "\n",
    "In \\[29\\]:\n",
    "\n",
    "    import time\n",
    "\n",
    "In \\[30\\]:\n",
    "\n",
    "    #Time for non-vectorized code:\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(cost(4, 5, 20, x_input, y_target))\n",
    "    t1 = time.time()\n",
    "    print(t1 - t0)\n",
    "\n",
    "    3182.406341674902\n",
    "    0.04597592353820801\n",
    "\n",
    "In \\[31\\]:\n",
    "\n",
    "    #Time for vectorized code:\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(cost_vectorized(4, 5, 20, x_input, y_target))\n",
    "    t1 = time.time()\n",
    "    print(t1 - t0)\n",
    "\n",
    "    3182.4063416749013\n",
    "    0.051526546478271484\n",
    "\n",
    "In \\[32\\]:\n",
    "\n",
    "    # add an extra feature (column in the input) that are just all ones\n",
    "    x_in = np.concatenate([x_input, np.ones([np.shape(x_input)[0], 1])], axis=1)\n",
    "    x_in\n",
    "\n",
    "Out\\[32\\]:\n",
    "\n",
    "    array([[ 2.31 ,  6.575,  1.   ],\n",
    "           [ 7.07 ,  6.421,  1.   ],\n",
    "           [ 7.07 ,  7.185,  1.   ],\n",
    "           ...,\n",
    "           [11.93 ,  6.976,  1.   ],\n",
    "           [11.93 ,  6.794,  1.   ],\n",
    "           [11.93 ,  6.03 ,  1.   ]])\n",
    "\n",
    "In \\[33\\]:\n",
    "\n",
    "    def solve_exactly(X, t):\n",
    "        '''\n",
    "        Solve linear regression exactly. (fully vectorized)\n",
    "        \n",
    "        Given `X` - NxD matrix of inputs\n",
    "              `t` - target outputs\n",
    "        Returns the optimal weights as a D-dimensional vector\n",
    "        '''\n",
    "        # TODO: write this!\n",
    "        N, D = np.shape(X)\n",
    "        A = np.matmul(X.T, X)\n",
    "        c = np.dot(X.T, t)\n",
    "        return np.matmul(np.linalg.inv(A), c)\n",
    "\n",
    "In \\[34\\]:\n",
    "\n",
    "    solve_exactly(x_in, y_target)\n",
    "\n",
    "Out\\[34\\]:\n",
    "\n",
    "    array([ -0.33471389,   7.82205511, -22.89831573])\n",
    "\n",
    "In \\[35\\]:\n",
    "\n",
    "    # Vectorized gradient function\n",
    "    def gradfn(weights, X, t):\n",
    "        '''\n",
    "        Given `weights` - a current \"Guess\" of what our weights should be\n",
    "              `X` - matrix of shape (N,D) of input features\n",
    "              `t` - target y values\n",
    "        Return gradient of each weight evaluated at the current value\n",
    "        '''\n",
    "        # TODO: write this!\n",
    "        N, D = np.shape(X)\n",
    "        y_pred = np.matmul(X, weights)\n",
    "        error = y_pred - t\n",
    "        return np.matmul(np.transpose(x_in), error) / float(N)\n",
    "\n",
    "In \\[36\\]:\n",
    "\n",
    "    #With this function, we can solve the optimization problem by repeatedly applying gradient descent on  w :\n",
    "\n",
    "    def solve_via_gradient_descent(X, t, print_every=5000,\n",
    "                                   niter=100000, alpha=0.005):\n",
    "        '''\n",
    "        Given `X` - matrix of shape (N,D) of input features\n",
    "              `t` - target y values\n",
    "        Solves for linear regression weights.\n",
    "        Return weights after `niter` iterations.\n",
    "        '''\n",
    "        # TODO: write this!\n",
    "        N, D = np.shape(X)\n",
    "        # initialize all the weights to zeros\n",
    "        w = np.zeros([D])\n",
    "        for k in range(niter):\n",
    "            dw = gradfn(w, X, t)\n",
    "            w = w - alpha*dw\n",
    "            if k % print_every == 0:\n",
    "                print('Weight after %d iteration: %s' % (k, str(w)))\n",
    "        return w\n",
    "\n",
    "In \\[37\\]:\n",
    "\n",
    "    solve_via_gradient_descent( X=x_in, t=y_target)\n",
    "\n",
    "    Weight after 0 iteration: [1.10241186 0.73047508 0.11266403]\n",
    "    Weight after 5000 iteration: [-0.48304613  5.10076868 -3.97899253]\n",
    "    Weight after 10000 iteration: [-0.45397323  5.63413678 -7.6871518 ]\n",
    "    Weight after 15000 iteration: [ -0.43059857   6.06296553 -10.66851736]\n",
    "    Weight after 20000 iteration: [ -0.41180532   6.40774447 -13.06553969]\n",
    "    Weight after 25000 iteration: [ -0.39669551   6.68494726 -14.9927492 ]\n",
    "    Weight after 30000 iteration: [ -0.38454721   6.90781871 -16.54222851]\n",
    "    Weight after 35000 iteration: [ -0.37477995   7.08700769 -17.78801217]\n",
    "    Weight after 40000 iteration: [ -0.36692706   7.23107589 -18.78962409]\n",
    "    Weight after 45000 iteration: [ -0.36061333   7.34690694 -19.59492155]\n",
    "    Weight after 50000 iteration: [ -0.35553708   7.44003528 -20.24238191]\n",
    "    Weight after 55000 iteration: [ -0.35145576   7.5149106  -20.762941  ]\n",
    "    Weight after 60000 iteration: [ -0.34817438   7.57511047 -21.18147127]\n",
    "    Weight after 65000 iteration: [ -0.34553614   7.62351125 -21.51797024]\n",
    "    Weight after 70000 iteration: [ -0.343415     7.66242555 -21.78851591]\n",
    "    Weight after 75000 iteration: [ -0.34170959   7.69371271 -22.00603503]\n",
    "    Weight after 80000 iteration: [ -0.34033844   7.71886763 -22.18092072]\n",
    "    Weight after 85000 iteration: [ -0.33923604   7.73909222 -22.32152908]\n",
    "    Weight after 90000 iteration: [ -0.3383497    7.75535283 -22.4345784 ]\n",
    "    Weight after 95000 iteration: [ -0.33763709   7.76842638 -22.52547023]\n",
    "\n",
    "Out\\[37\\]:\n",
    "\n",
    "    array([ -0.33706425,   7.77893565, -22.59853432])\n",
    "\n",
    "In \\[38\\]:\n",
    "\n",
    "    np.linalg.lstsq(x_in, y_target)\n",
    "\n",
    "    C:\\Users\\Madarasinghe\\AppData\\Local\\Temp\\ipykernel_1424\\1328349989.py:1: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
    "    To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
    "      np.linalg.lstsq(x_in, y_target)\n",
    "\n",
    "Out\\[38\\]:\n",
    "\n",
    "    (array([ -0.33471389,   7.82205511, -22.89831573]),\n",
    "     array([19807.614505]),\n",
    "     3,\n",
    "     array([318.75354429,  75.21961717,   2.10127199]))\n",
    "\n",
    "In \\[ \\]:"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
